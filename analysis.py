import pandas as pd
import numpy as np
import jieba
import re
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from snownlp import SnowNLP
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
import plotly.subplots as sp
import streamlit as st
from datetime import timedelta
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import warnings
warnings.filterwarnings('ignore')

# å…¨å±€æ ·å¼è®¾ç½®
plt.style.use('ggplot')
sns.set_palette(sns.color_palette(["#ff69b4", "#db7093"]))
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½® Plotly çš„æ¨¡æ¿
pio.templates.default = "plotly_white"

# å…¨å±€é¢œè‰²è®¾ç½®
COLORS = {
    'your_color': '#ff69b4',  # ä½ çš„é¢œè‰²
    'ta_color': '#db7093',    # TAçš„é¢œè‰²
    'wordcloud_color': 'RdPu',  # è¯äº‘é¢œè‰²
    'heatmap_color': 'reds',    # çƒ­åŠ›å›¾é¢œè‰²
}

class WeChatAnalyzer:
    def __init__(self, df, stopwords_path="CNstopwords.txt", your_name="ä½ ", ta_name="TA"):
        self.df = self._preprocess_data(df)
        self.stopwords = self._load_stopwords(stopwords_path)
        self.your_name = your_name
        self.ta_name = ta_name
        
    def _preprocess_data(self, df):
        df = df[df['Type'] == 1]
        df = df.dropna(subset=['StrContent'])
        df = df[~df['StrContent'].str.contains(r'https://www\.xiaohongshu\.com', na=False)]
        
        df['StrTime'] = pd.to_datetime(df['StrTime'])
        df['date'] = df['StrTime'].dt.date
        df['hour'] = df['StrTime'].dt.hour
        df['weekday'] = df['StrTime'].dt.weekday
        df['month'] = df['StrTime'].dt.month
        return df
    
    def _load_stopwords(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            return set([line.strip() for line in f])
    
    def generate_wordcloud(self, is_sender):
        texts = self.df[self.df['IsSender'] == is_sender]['StrContent']
        pattern = re.compile(r"\[.+?\]")
        
        words = []
        for text in texts:
            clean_text = pattern.sub('', str(text))
            words += [word for word in jieba.lcut(clean_text) 
                     if word not in self.stopwords and len(word) > 1]
        
        word_count = Counter(words)
        wc = WordCloud(font_path="simhei.ttf", 
                      background_color='white',
                      colormap=COLORS['wordcloud_color'],
                      width=1600, height=1200)
        wc.generate_from_frequencies(word_count)
        return wc, word_count.most_common(5)
    
    def plot_monthly_keywords(self):
        # å‡†å¤‡æ•°æ®
        results = []
        for is_sender in [1, 0]:
            df = self.df[self.df['IsSender'] == is_sender]
            monthly_data = {}
            
            # æŒ‰æœˆä»½å¤„ç†æ•°æ®
            for month, group in df.groupby(pd.Grouper(key='StrTime', freq='ME')):
                month_key = month.strftime("%Y-%m")
                texts = group['StrContent']
                pattern = re.compile(r"\[.+?\]")
                word_counter = Counter()
                
                for text in texts:
                    clean_text = pattern.sub('', str(text))
                    words = [word for word in jieba.lcut(clean_text) 
                            if word not in self.stopwords and len(word) > 1 and not re.search(r'[å˜»å“ˆå‘œ]', word)]
                    word_counter.update(words)
                
                # å­˜å‚¨æ¯ä¸ªæœˆçš„Top5
                monthly_data[month_key] = word_counter.most_common(5)
            
            # å°†æ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            all_months = sorted(monthly_data.keys())
            top_words = {i+1: [] for i in range(5)}  # å­˜å‚¨æ¯ä¸ªæ’åçš„æ•°æ®
            for month in all_months:
                words = monthly_data[month]
                for i in range(5):
                    if i < len(words):
                        top_words[i+1].append((month, words[i][0], words[i][1]))
                    else:
                        top_words[i+1].append((month, None, 0))
            
            results.append((all_months, top_words))
        
        # åˆ›å»ºä¸¤ä¸ªå›¾è¡¨
        your_fig = self._create_single_chart(results[0], self.your_name, COLORS['your_color'])
        ta_fig = self._create_single_chart(results[1], self.ta_name, COLORS['ta_color'])
        
        return your_fig, ta_fig

    def _create_single_chart(self, data, name, color):
        months, top_words = data
        fig = go.Figure()
        
        # æ·»åŠ æ•°æ®
        for rank in range(1, 6):
            x_vals = []
            y_vals = []
            text_vals = []
            for month, word, count in top_words[rank]:
                x_vals.append(month)
                y_vals.append(count)
                text_vals.append(f"{word}: {count}" if word else "")
            
            fig.add_trace(go.Bar(
                x=x_vals,
                y=y_vals,
                name=f"Top{rank}",
                text=text_vals,
                marker_color=color,
                opacity=0.9 - (rank-1)*0.15,
                hoverinfo="x+y+text"
            ))
        
        # æ›´æ–°å¸ƒå±€
        fig.update_layout(
            title=f"{name} çš„æœˆåº¦å…³é”®è¯åˆ†å¸ƒ",
            xaxis_title="æœˆä»½",
            yaxis_title="å‡ºç°æ¬¡æ•°",
            height=400,
            # legend=dict(
            #     orientation="h",
            #     yanchor="bottom",
            #     y=1.02,
            #     xanchor="right",
            #     x=1
            # ),
            margin=dict(t=80),
            hovermode="x unified"
        )
        
        # ä¼˜åŒ–xè½´æ˜¾ç¤º
        fig.update_xaxes(
            tickangle=45,
            tickmode='array',
            tickvals=months,
            ticktext=[m.split('-')[1] + 'æœˆ' for m in months]  # æ˜¾ç¤ºä¸º"01æœˆ","02æœˆ"
        )
        
        return fig

    def plot_calendar_heatmap(self, is_sender):
        df = self.df[self.df['IsSender'] == is_sender]
        daily_counts = df.groupby('date').size().reset_index(name='count')
        daily_counts['date'] = pd.to_datetime(daily_counts['date'])
        daily_counts['period'] = daily_counts['date'].dt.day.apply(
            lambda x: f"{(x-1)//4*4+1}-{((x-1)//4+1)*4}" if x < 29 else "29-31"
        )
        
        fig = px.density_heatmap(
            daily_counts,
            x='period',
            y=daily_counts['date'].dt.month_name(),
            z='count',
            histfunc="sum",
            color_continuous_scale=COLORS['heatmap_color']
        )
        fig.update_layout(
            yaxis_title="æœˆä»½",
            xaxis_title="æ—¥æœŸåŒºé—´",
            height=600
        )
        max_day = daily_counts.loc[daily_counts['count'].idxmax()]
        label = self.ta_name if is_sender == 0 else self.your_name
        analysis = f"ğŸ“… {label}åœ¨{max_day['date'].strftime('%Y-%m')}æœˆçš„{max_day['date'].day}æ—¥è¾¾åˆ°å•æ—¥å³°å€¼ï¼ˆ{max_day['count']}æ¡ï¼‰"
        return fig, analysis
    
    def analyze_response_time(self):
        # åˆ›å»ºä¸€ä¸ªæŒ‰ 'StrTime' åˆ—æ’åºçš„ä¸´æ—¶æ•°æ®æ¡†
        temp_df = self.df.sort_values('StrTime')
        
        # è®¡ç®—æ¯ä¸ªå‘é€è€…ä¹‹é—´çš„è¿ç»­æ¶ˆæ¯ä¹‹é—´çš„æ—¶é—´å·®ï¼ˆåˆ†é’Ÿï¼‰
        temp_df['time_diff'] = temp_df.groupby('IsSender')['StrTime'].diff().dt.total_seconds() / 60
        
        # è¿‡æ»¤å‡ºå‘é€è€…å‘ç”Ÿå˜åŒ–ä¸”æ—¶é—´å·®ä¸ºæ­£çš„è¡Œ
        response_df = temp_df[(temp_df['IsSender'].diff() != 0) & (temp_df['time_diff'] > 0)]
        
        # åˆ›å»ºä¸€ä¸ªç®±çº¿å›¾ï¼Œæ˜¾ç¤ºæ¯ä¸ªå‘é€è€…çš„å“åº”æ—¶é—´åˆ†å¸ƒ
        fig = px.box(response_df, x='IsSender', y='time_diff', 
                    color='IsSender',
                    color_discrete_map={0: COLORS['ta_color'], 1: COLORS['your_color']},
                    title='æ¶ˆæ¯å“åº”æ—¶é—´åˆ†å¸ƒåˆ†æ')
        
        # æ›´æ–°å›¾å½¢å¸ƒå±€ï¼Œè®¾ç½®è‡ªå®šä¹‰çš„ y è½´æ ‡é¢˜ã€x è½´æ ‡é¢˜ã€å›¾ä¾‹ç­‰
        fig.update_layout(
            yaxis_title='å“åº”æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰',
            xaxis_title='å‘é€è€…',
            showlegend=False,
            xaxis={'ticktext': [f'{self.ta_name}çš„å›å¤é€Ÿåº¦', f'{self.your_name}çš„å›å¤é€Ÿåº¦'], 'tickvals': [0, 1]}
        )
        
        # è®¡ç®—æ¯ä¸ªå‘é€è€…çš„å“åº”æ—¶é—´ä¸­ä½æ•°
        median_times = response_df.groupby('IsSender')['time_diff'].median()
        
        # ç”Ÿæˆä¸€ä¸ªåˆ†æå­—ç¬¦ä¸²ï¼ŒåŒ…å«æ¯ä¸ªå‘é€è€…çš„å“åº”æ—¶é—´ä¸­ä½æ•°
        analysis = f"â± {self.ta_name}çš„å›å¤ä¸­ä½æ—¶é—´ä¸º{median_times[0]:.1f}åˆ†é’Ÿï¼Œ{self.your_name}çš„å›å¤ä¸­ä½æ—¶é—´ä¸º{median_times[1]:.1f}åˆ†é’Ÿ"
        
        # è¿”å›å›¾å½¢å’Œåˆ†æå­—ç¬¦ä¸²
        return fig, analysis

    
    def _get_max_consecutive_days(self):
        dates = self.df['date'].unique()
        dates.sort()

        if len(dates) <= 1:
            return len(dates)
    
        diffs = np.diff(dates)
        consecutive = []
        count = 1

        for d in diffs:
            if d == timedelta(days=1):
                count +=1
            else:
                consecutive.append(count)
                count = 1
        consecutive.append(count)
        return max(consecutive) if consecutive else 0
    def plot_daily_trend(self):
        daily_counts = self.df.groupby('date').size()
        fig = go.Figure(data=[go.Scatter(x=daily_counts.index, y=daily_counts.values,
                                        mode='lines', line=dict(width=2, color=COLORS['your_color']))])
        fig.update_layout(xaxis_title="æ—¥æœŸ", yaxis_title="æ¶ˆæ¯é‡", height=400)
        max_day = daily_counts.idxmax().strftime('%Y-%m-%d')
        analysis = f"ğŸ“ˆ ä½ ä»¬çš„èŠå¤©é«˜å³°å‡ºç°åœ¨{max_day}ï¼Œå½“æ—¥å…±å‘é€{daily_counts.max()}æ¡æ¶ˆæ¯"
        return fig, analysis

    def plot_sentiment_analysis(self):
        self.df['sentiment'] = self.df['StrContent'].apply(
            lambda x: SnowNLP(str(x)).sentiments if len(str(x)) > 2 else 0.5)
        
        weekly_sentiment = self.df.groupby(['IsSender', pd.Grouper(key='StrTime', freq='W')])['sentiment'].mean().reset_index()
        
        fig = go.Figure()
        for is_sender in [0, 1]:
            subset = weekly_sentiment[weekly_sentiment['IsSender'] == is_sender]
            fig.add_trace(go.Scatter(
                x=subset['StrTime'],
                y=subset['sentiment'],
                mode='lines+markers',
                name=self.ta_name if is_sender == 0 else self.your_name,
                line=dict(color=COLORS['ta_color'] if is_sender == 0 else COLORS['your_color']) 
            ))
        
        fig.update_layout(
            xaxis_title="æ—¥æœŸ",
            yaxis_title="æƒ…æ„Ÿå€¼",
            height=400,
            legend=dict(orientation="h", yanchor="bottom", y=1.02)
        )
        avg_sentiment = weekly_sentiment.groupby('IsSender')['sentiment'].mean()
        analysis = f"ğŸ˜Š {self.ta_name}å¹³å‡æƒ…æ„Ÿå€¼{avg_sentiment[0]:.2f}ï¼Œ{self.your_name}çš„å¹³å‡æƒ…æ„Ÿå€¼{avg_sentiment[1]:.2f}"
        return fig, analysis

    def plot_hourly_distribution(self):
        hour_counts = self.df.groupby('hour').size()
        fig = go.Figure(data=[go.Bar(x=hour_counts.index, y=hour_counts.values,
                                    marker_color=COLORS['your_color'])])
        fig.update_layout(xaxis_title="å°æ—¶", yaxis_title="æ¶ˆæ¯é‡", height=400)
        peak_hour = hour_counts.idxmax()
        analysis = f"ğŸŒ™ ä½ ä»¬èŠå¤©ä¸­æœ€æ´»è·ƒçš„æ—¶æ®µä¸º{peak_hour}ç‚¹ï¼Œå…±å‘é€{hour_counts.max()}æ¡æ¶ˆæ¯"
        return fig, analysis

    def get_metrics(self):
        metrics = {}
        metrics['msg_count'] = self.df.groupby('IsSender').size().to_dict()
        metrics['peak_hour'] = self.df.groupby('hour').size().idxmax()
        metrics['max_consecutive_days'] = self._get_max_consecutive_days()
        
        # æœ€å¸¸èŠå¤©å¤©æ•°ï¼ˆæ¶ˆæ¯é‡>50çš„å¤©æ•°ï¼‰
        daily_counts = self.df.groupby('date').size()
        metrics['active_days'] = len(daily_counts[daily_counts > 50])
        return metrics

    def analyze_joint_topics(self, n_topics=2, top_words=5):
        """ä½¿ç”¨NMFè¿›è¡Œè”åˆä¸»é¢˜åˆ†æ"""
        monthly_data = []

        extra_stopwords = set([self.your_name, self.ta_name])  # æ˜ç¡®è¿‡æ»¤åŒæ–¹æ˜µç§°
        invalid_chars = {'å˜»', 'å“ˆ', 'å‘œ', 'å•¦', 'å“¦'}  # æ–°å¢æ— æ•ˆå­—ç¬¦é»‘åå•

        # æŒ‰æœˆä»½åˆå¹¶æ‰€æœ‰å¯¹è¯
        for month, group in self.df.groupby(pd.Grouper(key='StrTime', freq='ME')):
            # åˆå¹¶å½“æœˆæ‰€æœ‰æ¶ˆæ¯
            combined_text = ' '.join(group['StrContent'].astype(str))
            # æ¸…æ´—æ–‡æœ¬
            combined_text = re.sub(r"\[.+?\]", "", combined_text)
            words = [word for word in jieba.lcut(combined_text) 
                if (word not in self.stopwords) and 
                (len(word) > 1) and 
                (word not in extra_stopwords) and 
                (not any(char in invalid_chars for char in word))]
            monthly_data.append({
                'month': month.strftime("%Y-%m"),
                'text': ' '.join(words),
                'msg_count': len(group)
            })
        
        # åˆ›å»ºTF-IDFçŸ©é˜µ
        tfidf = TfidfVectorizer(
            max_features=1000,
            token_pattern=r'(?u)\b\w{2,}\b',  # ä»…ä¿ç•™2å­—ä»¥ä¸Šè¯è¯­
            ngram_range=(1,2)  # å…è®¸äºŒå…ƒè¯ç»„
        )
        dtm = tfidf.fit_transform([d['text'] for d in monthly_data])
        
        # è®­ç»ƒNMFæ¨¡å‹
        nmf = NMF(
            n_components=n_topics,
            random_state=42,
            beta_loss='kullback-leibler',  # æ›´é€‚åˆçŸ­æ–‡æœ¬
            solver='mu',
            max_iter=1000
        )
        nmf.fit(dtm)
        
        # æå–ç‰¹å¾è¯
        feature_names = tfidf.get_feature_names_out()
        topics = []
        for topic_idx, topic in enumerate(nmf.components_):
            top_features = [feature_names[i] for i in topic.argsort()[:-top_words-1:-1]]
            topics.append({
                'topic_id': topic_idx+1,
                'keywords': 'ã€'.join(top_features)
            })
        
        # åˆ†é…ä¸»é¢˜åˆ°æœˆä»½
        topic_results = []
        for i, month_data in enumerate(monthly_data):
            weights = nmf.transform(dtm[i])[0]
            main_topic = weights.argmax() + 1
            topic_results.append({
                'month': month_data['month'],
                'msg_count': month_data['msg_count'],
                'main_topic': main_topic,
                'keywords': topics[main_topic-1]['keywords'],
                'weight': weights.max()
            })
        
        return pd.DataFrame(topic_results)
    
    def get_longest_chat_duration(self):
        """è®¡ç®—æœ€é•¿çš„ä¸€æ¬¡èŠå¤©æŒç»­æ—¶é—´"""
        temp_df = self.df.sort_values('StrTime')
        temp_df['time_diff'] = temp_df['StrTime'].diff().dt.total_seconds() / 60
        temp_df['chat_id'] = (temp_df['time_diff'] > 10).cumsum()  # è®¾è¶…è¿‡10åˆ†é’Ÿçš„é—´éš”ä¸ºæ–°èŠå¤©
        
        chat_durations = temp_df.groupby('chat_id')['StrTime'].agg(['min', 'max'])
        chat_durations['duration'] = (chat_durations['max'] - chat_durations['min']).dt.total_seconds() / 60
        
        longest_chat = chat_durations.loc[chat_durations['duration'].idxmax()]
        return longest_chat['duration'], longest_chat['min'], longest_chat['max']

# Streamlit App
st.title("ğŸ’¬ WeChat èŠå¤©è®°å½•åˆ†æ")

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
.subheader {
        margin-bottom: 4rem; 
}
.analysis-text {
    font-size: 16px !important;
    color: #DB7093 !important;
}
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ– session_state
if 'hide_names' not in st.session_state:
    st.session_state.hide_names = False
if 'uploaded_file' not in st.session_state:
    st.session_state.uploaded_file = None

# æ˜µç§°è¾“å…¥æ¡†
if not st.session_state.hide_names:
    col_name1, col_name2 = st.columns(2)
    with col_name1:
        your_name = st.text_input("TAå¦‚ä½•ç§°å‘¼ä½ ", value="ä½ ")
    with col_name2:
        ta_name = st.text_input("ä½ å¦‚ä½•ç§°å‘¼TA", value="TA")

    # æ·»åŠ ä¸€ä¸ªæŒ‰é’®æ¥éšè—è¾“å…¥æ¡†
    if st.button("ç¡®è®¤"):
        st.session_state.hide_names = True
        st.session_state.your_name = your_name
        st.session_state.ta_name = ta_name
        st.rerun()
else:
    your_name = st.session_state.your_name
    ta_name = st.session_state.ta_name

# æ–‡ä»¶ä¸Šä¼ æ¡†
uploaded_file = st.file_uploader("è¯·ä¸Šä¼ å¾®ä¿¡èŠå¤©è®°å½•CSVæ–‡ä»¶", type="csv", help="éœ€è¦é¦–å…ˆä»[memotrace](https://memotrace.cn/doc/posts/deploy/install.html)è·å–csvæ–‡ä»¶")

if uploaded_file is not None:

    df = pd.read_csv(uploaded_file)
    analyzer = WeChatAnalyzer(df, your_name=your_name, ta_name=ta_name)
    
    # å…³é”®æŒ‡æ ‡
    st.subheader("ğŸ‘€ æ ¸å¿ƒæ•°æ®æ¦‚è§ˆ")
    metrics = analyzer.get_metrics()
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric(label=f"{ta_name}çš„æ¶ˆæ¯é‡", value=metrics['msg_count'].get(0, 0))
    with col2:
        st.metric(label=f"{your_name}çš„æ¶ˆæ¯é‡", value=metrics['msg_count'].get(1, 0))
    with col3:
        st.metric(label="æœ€é•¿è¿ç»­å¤©æ•°", value=metrics['max_consecutive_days'])
    with col4:
        st.metric(label="ç«çƒ­èŠå¤©å¤©æ•°", value=metrics['active_days'], 
                help="å•æ—¥æ¶ˆæ¯é‡è¶…è¿‡50æ¡çš„å¤©æ•°")

    # æœ€é•¿èŠå¤©æŒç»­æ—¶é—´
    longest_duration, start_time, end_time = analyzer.get_longest_chat_duration()
    st.markdown(f"**ä½ ä»¬æœ€é•¿çš„ä¸€æ¬¡èŠå¤©æŒç»­äº† {longest_duration:.1f} åˆ†é’Ÿ**ï¼Œä» {start_time.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time.strftime('%Y-%m-%d %H:%M')}.")
    st.markdown("æ˜¯åœ¨è°ˆè®ºä»€ä¹ˆè¯é¢˜å‘¢ï¼Ÿ")
    
    # å…³é”®è¯åˆ†æ
    st.subheader("â˜ï¸ è¯äº‘")
    col1, col2 = st.columns(2)
    with col1:
        wc_ta, top_words_ta = analyzer.generate_wordcloud(0)
        plt.figure(figsize=(8, 5))
        plt.imshow(wc_ta, interpolation='bilinear')
        plt.axis('off')
        st.pyplot(plt)
        st.markdown(f"<div class='analysis-text'>{ta_name}çš„é«˜é¢‘è¯ï¼š{'ã€'.join([w[0] for w in top_words_ta])}</div>", unsafe_allow_html=True)
    
    with col2:
        wc_my, top_words_my = analyzer.generate_wordcloud(1)
        plt.figure(figsize=(8, 5))
        plt.imshow(wc_my, interpolation='bilinear')
        plt.axis('off')
        st.pyplot(plt)
        st.markdown(f"<div class='analysis-text'>{your_name}çš„é«˜é¢‘è¯ï¼š{'ã€'.join([w[0] for w in top_words_my])}</div>", unsafe_allow_html=True)

    def get_topic_data(_analyzer):
        return _analyzer.analyze_joint_topics()

    topic_df = get_topic_data(analyzer)

    # # åˆ›å»ºå¯è§†åŒ–
    # fig = px.scatter(
    #     topic_df,
    #     x='month',
    #     y='weight',
    #     size='msg_count',
    #     color='main_topic',
    #     hover_name='keywords',
    #     color_continuous_scale=COLORS['heatmap_color'],
    #     size_max=20,
    #     labels={'month':'æœˆä»½', 'weight':'ä¸»é¢˜å¼ºåº¦', 'msg_count':'æ¶ˆæ¯é‡'}
    # )

    # fig.update_layout(
    #     height=500,
    #     xaxis=dict(tickangle=45),
    #     hoverlabel=dict(bgcolor="white")
    # )

    # st.plotly_chart(fig, use_container_width=True)

    st.markdown("  ")

    # æ·»åŠ è¯¦ç»†è§£è¯»
    st.subheader("ğŸ§© æœˆåº¦ä¸»é¢˜åˆ†æ")
    for idx, row in topic_df.iterrows():
        st.markdown(f"""
        **{row['month']}æœˆ**  
        ğŸ”– ä¸»è¦è¯é¢˜ï¼š{row['keywords']}  
        ğŸ“Š æ¶ˆæ¯é‡ï¼š{row['msg_count']}æ¡ | ä¸»é¢˜å¼ºåº¦ï¼š{row['weight']:.2f}
        """)
        st.progress(row['weight'])

    # æœˆä»½é«˜é¢‘è¯
    # st.markdown("### ğŸ“Š æœˆåº¦å…³é”®è¯")

    # ä½ çš„å…³é”®è¯åˆ†å¸ƒ
    your_fig, _ = analyzer.plot_monthly_keywords()
    st.plotly_chart(your_fig, use_container_width=True)

    # TAçš„å…³é”®è¯åˆ†å¸ƒ
    _, ta_fig = analyzer.plot_monthly_keywords()
    st.plotly_chart(ta_fig, use_container_width=True)

    # æ¯æ—¥è¶‹åŠ¿
    st.subheader("ğŸ“… èŠå¤©è¶‹åŠ¿åˆ†æ")
    daily_trend, trend_analysis = analyzer.plot_daily_trend()
    st.plotly_chart(daily_trend)
    st.markdown(f"`{trend_analysis}`")

    # æ—¥å†çƒ­åŠ›å›¾
    st.subheader("ğŸ—“ï¸ æ´»è·ƒåº¦åˆ†å¸ƒ")
    col3, col4 = st.columns(2)
    with col3:
        heatmap_ta, analysis_ta = analyzer.plot_calendar_heatmap(0)
        st.plotly_chart(heatmap_ta)
        st.markdown(f"`{analysis_ta}`")
    
    with col4:
        heatmap_my, analysis_my = analyzer.plot_calendar_heatmap(1)
        st.plotly_chart(heatmap_my)
        st.markdown(f"`{analysis_my}`")

    # å“åº”æ—¶é—´åˆ†æ
    st.subheader("â³ å“åº”æ—¶é—´åˆ†æ")
    response_time_fig, response_analysis = analyzer.analyze_response_time()
    st.plotly_chart(response_time_fig)
    st.markdown(f"`{response_analysis}`")

    # æƒ…æ„Ÿåˆ†æ
    st.subheader("ğŸ˜Š æƒ…æ„Ÿåˆ†æ")
    sentiment_fig, sentiment_analysis = analyzer.plot_sentiment_analysis()
    st.plotly_chart(sentiment_fig)
    st.markdown(f"`{sentiment_analysis}`")

    # æ—¶æ®µåˆ†æ
    st.subheader("â° èŠå¤©æ—¶æ®µåˆ†å¸ƒ")
    hourly_fig, hourly_analysis = analyzer.plot_hourly_distribution()
    st.plotly_chart(hourly_fig)
    st.markdown(f"`{hourly_analysis}`")