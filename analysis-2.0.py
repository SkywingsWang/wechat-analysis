import pandas as pd
import jieba
import re
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from snownlp import SnowNLP
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
import streamlit as st

# å…¨å±€æ ·å¼è®¾ç½®
plt.style.use('ggplot')
sns.set_palette(sns.color_palette(["#ff69b4", "#db7093"]))
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½® Plotly çš„æ¨¡æ¿
pio.templates.default = "plotly_white"

class WeChatAnalyzer:
    def __init__(self, df, stopwords_path="CNstopwords.txt"):
        self.df = self._preprocess_data(df)
        self.stopwords = self._load_stopwords(stopwords_path)
        
    def _preprocess_data(self, df):
        df = df[df['Type'] == 1]
        df = df.dropna(subset=['StrContent'])
        df = df[~df['StrContent'].str.contains(r'https://www\.xiaohongshu\.com', na=False)]
        
        df['StrTime'] = pd.to_datetime(df['StrTime'])
        df['date'] = df['StrTime'].dt.date
        df['hour'] = df['StrTime'].dt.hour
        df['weekday'] = df['StrTime'].dt.weekday
        df['month'] = df['StrTime'].dt.month
        return df
    
    def _load_stopwords(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            return set([line.strip() for line in f])
    
    def generate_wordcloud(self, is_sender):
        texts = self.df[self.df['IsSender'] == is_sender]['StrContent']
        pattern = re.compile(r"\[.+?\]")
        
        words = []
        for text in texts:
            clean_text = pattern.sub('', str(text))
            words += [word for word in jieba.lcut(clean_text) 
                     if word not in self.stopwords and len(word) > 1]
        
        word_count = Counter(words)
        wc = WordCloud(font_path="simhei.ttf", 
                      background_color='white',
                      colormap='RdPu',
                      width=1600, height=1200)
        wc.generate_from_frequencies(word_count)
        return wc, word_count.most_common(5)
    
    def plot_calendar_heatmap(self, is_sender):
        df = self.df[self.df['IsSender'] == is_sender]
        daily_counts = df.groupby('date').size().reset_index(name='count')
        daily_counts['date'] = pd.to_datetime(daily_counts['date'])
        
        fig = px.density_heatmap(
            daily_counts,
            x=daily_counts['date'].dt.day,
            y=daily_counts['date'].dt.month_name(),
            z='count',
            histfunc="sum",
            color_continuous_scale='reds'
        )
        fig.update_layout(
            yaxis_title="æœˆä»½",
            xaxis_title="æ—¥æœŸ",
            height=600
        )
        max_day = daily_counts.loc[daily_counts['count'].idxmax()]
        
        # Determine the label based on is_sender
        label = "TA" if is_sender == 0 else "ä½ "
        analysis = f"ğŸ“… {label}åœ¨{max_day['date'].strftime('%Y-%m')}æœˆçš„{max_day['date'].day}æ—¥è¾¾åˆ°å•æ—¥å³°å€¼ï¼ˆ{max_day['count']}æ¡ï¼‰"
        return fig, analysis
    
    def analyze_response_time(self):
        temp_df = self.df.sort_values('StrTime')
        temp_df['time_diff'] = temp_df.groupby('IsSender')['StrTime'].diff().dt.total_seconds() / 60
        response_df = temp_df[(temp_df['IsSender'].diff() != 0) & (temp_df['time_diff'] > 0)]
        
        fig = px.box(response_df, x='IsSender', y='time_diff', 
                     color='IsSender',
                     color_discrete_map={0: "#ff69b4", 1: "#db7093"},
                     title='æ¶ˆæ¯å“åº”æ—¶é—´åˆ†å¸ƒåˆ†æ')
        fig.update_layout(
            yaxis_title='å“åº”æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰',
            xaxis_title='å‘é€è€…',
            showlegend=False,
            xaxis={'ticktext': ['TAçš„å›å¤é€Ÿåº¦', 'ä½ çš„å›å¤é€Ÿåº¦'], 'tickvals': [0, 1]}
        )
        median_times = response_df.groupby('IsSender')['time_diff'].median()
        analysis = f"â± TAçš„å›å¤ä¸­ä½æ—¶é—´ä¸º{median_times[0]:.1f}åˆ†é’Ÿï¼Œä½ çš„å›å¤ä¸­ä½æ—¶é—´ä¸º{median_times[1]:.1f}åˆ†é’Ÿ"
        return fig, analysis
    
    def plot_daily_trend(self):
        daily_counts = self.df.groupby('date').size()
        fig = go.Figure(data=[go.Scatter(x=daily_counts.index, y=daily_counts.values,
                                        mode='lines', line=dict(width=2, color='#ff69b4'))])
        fig.update_layout(xaxis_title="æ—¥æœŸ", yaxis_title="æ¶ˆæ¯é‡", height=400)
        max_day = daily_counts.idxmax().strftime('%Y-%m-%d')
        analysis = f"ğŸ“ˆ ä½ ä»¬çš„èŠå¤©é«˜å³°å‡ºç°åœ¨{max_day}ï¼Œå½“æ—¥å…±å‘é€{daily_counts.max()}æ¡æ¶ˆæ¯"
        return fig, analysis

    def plot_sentiment_analysis(self):
        self.df['sentiment'] = self.df['StrContent'].apply(
            lambda x: SnowNLP(str(x)).sentiments if len(str(x)) > 2 else 0.5)
        weekly_sentiment = self.df.resample('W', on='StrTime')['sentiment'].mean()
        fig = go.Figure(data=[go.Scatter(x=weekly_sentiment.index, y=weekly_sentiment.values,
                                        mode='lines+markers', line=dict(color='#db7093'))])
        fig.update_layout(xaxis_title="æ—¥æœŸ", yaxis_title="æƒ…æ„Ÿå€¼", height=400)
        avg_sentiment = weekly_sentiment.mean()
        analysis = f"ğŸ˜Š ä½ ä»¬èŠå¤©ä¸­çš„å¹³å‡æƒ…æ„Ÿå€¼ä¸º{avg_sentiment:.2f}ï¼ˆ1ä¸ºç§¯æï¼Œ0ä¸ºæ¶ˆæï¼‰ï¼Œè¿‘æœŸè¶‹åŠ¿ï¼š{'ä¸Šå‡' if weekly_sentiment.iloc[-1] > weekly_sentiment.iloc[-2] else 'ä¸‹é™'}"
        return fig, analysis

    def plot_hourly_distribution(self):
        hour_counts = self.df.groupby('hour').size()
        fig = go.Figure(data=[go.Bar(x=hour_counts.index, y=hour_counts.values,
                                    marker_color='#ff69b4')])
        fig.update_layout(xaxis_title="å°æ—¶", yaxis_title="æ¶ˆæ¯é‡", height=400)
        peak_hour = hour_counts.idxmax()
        analysis = f"ğŸŒ™ ä½ ä»¬èŠå¤©ä¸­æœ€æ´»è·ƒçš„æ—¶æ®µä¸º{peak_hour}ç‚¹ï¼Œå…±å‘é€{hour_counts.max()}æ¡æ¶ˆæ¯"
        return fig, analysis

    def get_metrics(self):
        metrics = {}
        metrics['msg_count'] = self.df.groupby('IsSender').size().to_dict()
        metrics['peak_hour'] = self.df.groupby('hour').size().idxmax()
        
        # æœ€å¸¸èŠå¤©å¤©æ•°ï¼ˆæ¶ˆæ¯é‡>50çš„å¤©æ•°ï¼‰
        daily_counts = self.df.groupby('date').size()
        metrics['active_days'] = len(daily_counts[daily_counts > 50])
        return metrics


# Streamlit App
st.title("ğŸ’¬ WeChat èŠå¤©è®°å½•åˆ†æ")

# File upload
uploaded_file = st.file_uploader("è¯·ä¸Šä¼ å¾®ä¿¡èŠå¤©è®°å½•CSVæ–‡ä»¶", type="csv", help="éœ€è¦é¦–å…ˆä»[memotrace](https://memotrace.cn/doc/posts/deploy/install.html)è·å–csvæ–‡ä»¶")
if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    analyzer = WeChatAnalyzer(df)
    
    # å…³é”®æŒ‡æ ‡
    st.subheader("ğŸ“Š æ ¸å¿ƒæ•°æ®æ¦‚è§ˆ")
    metrics = analyzer.get_metrics()
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(label="TAçš„æ¶ˆæ¯é‡", value=metrics['msg_count'].get(0, 0))
    with col2:
        st.metric(label="ä½ çš„æ¶ˆæ¯é‡", value=metrics['msg_count'].get(1, 0))
    with col3:
        st.metric(label="ç«çƒ­èŠå¤©å¤©æ•°", value=metrics['active_days'], 
                help="å•æ—¥æ¶ˆæ¯é‡è¶…è¿‡50æ¡çš„å¤©æ•°")

    # è¯äº‘å›¾
    st.subheader("ğŸ”¤ å…³é”®è¯åˆ†æ")
    col1, col2 = st.columns(2)
    with col1:
        wc_ta, top_words_ta = analyzer.generate_wordcloud(0)
        plt.figure(figsize=(8, 5))
        plt.imshow(wc_ta, interpolation='bilinear')
        plt.axis('off')
        st.pyplot(plt)
        st.caption(f"TAçš„é«˜é¢‘è¯ï¼š{'ã€'.join([w[0] for w in top_words_ta])}")
    
    with col2:
        wc_my, top_words_my = analyzer.generate_wordcloud(1)
        plt.figure(figsize=(8, 5))
        plt.imshow(wc_my, interpolation='bilinear')
        plt.axis('off')
        st.pyplot(plt)
        st.caption(f"ä½ çš„é«˜é¢‘è¯ï¼š{'ã€'.join([w[0] for w in top_words_my])}")

    # æ¯æ—¥è¶‹åŠ¿
    st.subheader("ğŸ“… èŠå¤©è¶‹åŠ¿åˆ†æ")
    daily_trend, trend_analysis = analyzer.plot_daily_trend()
    st.plotly_chart(daily_trend)
    st.markdown(f"`{trend_analysis}`")

    # æ—¥å†çƒ­åŠ›å›¾
    st.subheader("ğŸ—“ï¸ æ´»è·ƒåº¦åˆ†å¸ƒ")
    col3, col4 = st.columns(2)
    with col3:
        heatmap_ta, analysis_ta = analyzer.plot_calendar_heatmap(0)
        st.plotly_chart(heatmap_ta)
        st.markdown(f"`{analysis_ta}`")
    
    with col4:
        heatmap_my, analysis_my = analyzer.plot_calendar_heatmap(1)
        st.plotly_chart(heatmap_my)
        st.markdown(f"`{analysis_my}`")

    # å“åº”æ—¶é—´åˆ†æ
    st.subheader("â³ å“åº”æ—¶é—´åˆ†æ")
    response_time_fig, response_analysis = analyzer.analyze_response_time()
    st.plotly_chart(response_time_fig)
    st.markdown(f"`{response_analysis}`")

    # æƒ…æ„Ÿåˆ†æ
    st.subheader("ğŸ˜Š æƒ…æ„Ÿåˆ†æ")
    sentiment_fig, sentiment_analysis = analyzer.plot_sentiment_analysis()
    st.plotly_chart(sentiment_fig)
    st.markdown(f"`{sentiment_analysis}`")

    # æ—¶æ®µåˆ†æ
    st.subheader("â° èŠå¤©æ—¶æ®µåˆ†å¸ƒ")
    hourly_fig, hourly_analysis = analyzer.plot_hourly_distribution()
    st.plotly_chart(hourly_fig)
    st.markdown(f"`{hourly_analysis}`")